{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from data import preproc_a as preproc\n",
    "from convert_to_onnx_original import RetinaStaticExportWrapper, RetinaStaticExportWrapperV2\n",
    "from models.retinaface import UNetRetinaConcat\n",
    "from data.custom_dataset import GroupeAlignedDetectionDataset\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MODE = \"groupe_aligned\"\n",
    "num_classes = 11\n",
    "use_batch_normalization = True\n",
    "bounding_box_from_points = False\n",
    "multiclass = True\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\"name\": \"mobilenet0.25\",\n",
    "       \"pretrain\": False,\n",
    "       \"return_layers\": {\"stage1\": 1, \"stage2\": 2, \"stage3\": 3},\n",
    "       \"in_channel\": 32,\n",
    "       \"out_channel\": 64,\n",
    "       \"nms_threshold\": 0.35,\n",
    "       \"confidence_threshold\": -0.02,\n",
    "       \"image_size\": 640,\n",
    "       \"top_k\": 512,\n",
    "       \"min_sizes\": [[16, 32], [64, 128], [256, 512]],\n",
    "       \"steps\": [8, 16, 32],\n",
    "       \"variance\": [0.1, 0.2],\n",
    "       \"clip\": False,\n",
    "       \"mean\": (0, 0, 0)}\n",
    "\n",
    "img_dim = cfg[\"image_size\"]\n",
    "rgb_mean = cfg[\"mean\"]\n",
    "\n",
    "model = UNetRetinaConcat(cfg=cfg, use_batch_normalization=use_batch_normalization, num_classes=num_classes)\n",
    "export_model = RetinaStaticExportWrapperV2(model, cfg, bounding_box_from_points, return_mask=True)\n",
    "\n",
    "export_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(export_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat/dataframe_year_digit_2025.02.15_fix.csv.zip\")\n",
    "df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat_year/dataframe_year_digit_2025.03.10_fix.csv.zip\")\n",
    "_Dataset = GroupeAlignedDetectionDataset\n",
    "transform_train = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = _Dataset(df_train, preproc(img_dim, rgb_mean, use_mirror=False, pre_scales=[1.0], transform=transform_train), multiclass=multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute IoU\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "\n",
    "    xi1 = max(x1, x1g)\n",
    "    yi1 = max(y1, y1g)\n",
    "    xi2 = min(x2, x2g)\n",
    "    yi2 = min(y2, y2g)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _j in range(1024):\n",
    "    for _k in range(len(dataset_train)): \n",
    "\n",
    "        frame_data, mask_ground_truth, ground_truth = dataset_train[_k]\n",
    "        mask_ground_truth = torch.tensor(mask_ground_truth)\n",
    "        ground_truth = torch.tensor(ground_truth)\n",
    "        ground_truth[:, :-1] = (ground_truth[:, :-1].reshape(ground_truth.shape[0], -1, 2) * \\\n",
    "                                torch.tensor(frame_data.shape[1:][::-1])).reshape(ground_truth[:, :-1].shape)\n",
    "        output = export_model(torch.permute(frame_data[None, ...], (0, 2, 3, 1)).to(device))\n",
    "\n",
    "        confidence_list, landmark_list, bbox_lsit, mask_predicted = \\\n",
    "            [item.detach().cpu().numpy() for item in output]\n",
    "\n",
    "        frame_data = torch.permute(frame_data, (1, 2, 0)).contiguous().detach().byte().cpu().numpy()\n",
    "        mask_ground_truth = (255 * torch.nn.functional.one_hot(mask_ground_truth.long()))[0].detach().cpu().numpy()\n",
    "        mask_predicted = (255 * np.transpose(mask_predicted, (0, 2, 3, 1))[0]).astype(np.uint8)\n",
    "\n",
    "        if _k % 16 == 0:\n",
    "            for confidence, box, landmark in zip(confidence_list, bbox_lsit, landmark_list):\n",
    "                label_index = int(confidence.argmax().item())\n",
    "                label_confidence = confidence[label_index].item()\n",
    "                label_show = int(max(label_index - 1, 0))\n",
    "\n",
    "                _box = list(map(int, box + 0.5))\n",
    "                _landm = list(map(int, landmark + 0.5))\n",
    "                cv2.rectangle(frame_data, (_box[0], _box[1]), (_box[2], _box[3]), (0, 0, 255), 2)\n",
    "\n",
    "                cx = _box[0]\n",
    "                cy = _box[1] + 12\n",
    "                text = f\"{label_confidence:.4f}\"\n",
    "                cv2.putText(frame_data, text, (cx, cy),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 0.75, (255, 255, 0), thickness=2)\n",
    "\n",
    "                cx = _box[0]\n",
    "                cy = _box[1] + 48\n",
    "                text = f\"[{label_show}]\"\n",
    "                cv2.putText(frame_data, text, (cx, cy),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 0.75, (196, 196, 0), thickness=2)\n",
    "\n",
    "                # landms\n",
    "                cv2.circle(frame_data, (_landm[0], _landm[1]), 1, (0, 0, 255), 4)\n",
    "                cv2.circle(frame_data, (_landm[2], _landm[3]), 1, (0, 255, 255), 4)\n",
    "                cv2.circle(frame_data, (_landm[4], _landm[5]), 1, (255, 0, 255), 4)\n",
    "                cv2.circle(frame_data, (_landm[6], _landm[7]), 1, (0, 255, 0), 4)\n",
    "                cv2.circle(frame_data, (_landm[8], _landm[9]), 1, (255, 0, 0), 4)\n",
    "\n",
    "            frame_show = np.hstack([frame_data, mask_ground_truth, mask_predicted])\n",
    "\n",
    "            clear_output()\n",
    "            plt.imshow(frame_show)\n",
    "            plt.show()\n",
    "\n",
    "        p_true = ground_truth[:, 4:14].reshape(-1, 1, 5, 2)\n",
    "        p_pred = output[1].reshape(1, -1, 5, 2)\n",
    "\n",
    "        loss_matrix = torch.square(p_true - p_pred).sum((-1, -2))\n",
    "\n",
    "        cost_matrix = loss_matrix.detach()\n",
    "\n",
    "        true_selection, pred_selection = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        loss_class = torch.nn.functional.cross_entropy(output[0][pred_selection], ground_truth[:len(output[0]), -1].long())\n",
    "        loss_points = loss_matrix[true_selection, pred_selection].mean() / (640 * 640)\n",
    "        loss_negative_class = torch.nn.functional.cross_entropy(output[0], torch.zeros(len(output[0], ), dtype=torch.long), reduction='none')\n",
    "        loss_negative_class[pred_selection] = 0 \n",
    "        loss_negative_class = loss_negative_class.mean()\n",
    "        loss_iou = 1 - sum([compute_iou(a, b) for a, b in zip(ground_truth[true_selection, 0:4], output[2][pred_selection, :])]) / len(ground_truth)\n",
    "\n",
    "        loss = (loss_points + loss_class + loss_negative_class + loss_iou)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            break\n",
    "\n",
    "        if _k % 16 == 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "\n",
    "        print(_j, _k, loss_points.item(), loss_class.item(), loss_negative_class.item(), loss_iou.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f73cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01da64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(_k, loss_points.item(), loss_class.item(), loss_negative_class.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad647408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_selection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4de65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67feb7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
    "        # loss_j = criterion_unet_b(mask_predicted, mask_true[:, 0, ...].long()) + criterion_unet_j(mask_predicted[:, 1:, ...], torch.clamp(mask_true.long() - 1, 0, 1))\n",
    "        loss_j = criterion_unet_j(mask_predicted, mask_true.long())\n",
    "        loss_b = criterion_unet_b(mask_predicted, mask_true[:, 0, ...].long())\n",
    "\n",
    "        loss = cfg['loc_weight'] * loss_l + 10 * loss_c + loss_landm + weight_j * loss_j + weight_b * loss_b\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "p_true = ground_truth[:, 4:14].reshape(-1, 1, 5, 2)\n",
    "p_pred = output[1].reshape(1, -1, 5, 2)\n",
    "\n",
    "loss_matrix = torch.square(p_true - p_pred).sum((-1, -2))\n",
    "\n",
    "cost_matrix = -loss_matrix.detach()\n",
    "\n",
    "true_selection, pred_selection = scipy.optimize.linear_sum_assignment(cost_matrix)\n",
    "\n",
    "loss_points = loss_matrix[true_selection, pred_selection].mean() / 640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec1f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_true[true_selection] - p_pred[pred_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.take_along_dim(p_pred, torch.tensor(pred_selection), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd10682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ec638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred[pred_selection].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    elif\n",
    "\n",
    "\n",
    "        weight_j = 10\n",
    "        weight_b = 10\n",
    "\n",
    "        df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat/dataframe_year_digit_2025.02.15_fix.csv.zip\")\n",
    "        df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat_year/dataframe_year_digit_2025.03.10_fix.csv.zip\")\n",
    "        _Dataset = GroupeAlignedDetectionDataset\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    use_batch_normalization = True\n",
    "    multiclass = num_classes > 2\n",
    "\n",
    "    if use_batch_normalization:\n",
    "        rgb_mean = (0, 0, 0)\n",
    "    else:\n",
    "        rgb_mean = (104, 117, 123) # bgr order\n",
    "\n",
    "    save_folder = os.path.join(os.path.abspath(save_folder), '', DATA_MODE)\n",
    "\n",
    "    cfg['pretrain'] = False\n",
    "    max_epoch = 2048\n",
    "    device = 'cpu'\n",
    "    num_gpu = -1\n",
    "\n",
    "    print(\"Printing net...\")\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61267850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "import os\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import torch.utils.data as data\n",
    "from convert_to_onnx_original import RetinaStaticExportWrapper, RetinaStaticExportWrapperV2\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from data import preproc_a as preproc\n",
    "from data.custom_dataset import CustomDetectionDataset, GroupeAlignedDetectionDataset, GroupeDetectionDataset, GroupeAlignedMulticlassDetectionDataset\n",
    "from layers.modules import MultiBoxLoss\n",
    "from layers.functions.prior_box import PriorBox\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "from models.retinaface import UNetRetinaConcat\n",
    "# from models.retinaface import RetinaFace\n",
    "from segmentation_models_pytorch.losses.jaccard import JaccardLoss\n",
    "\n",
    "DATA_MODE = 'single'\n",
    "DATA_MODE = 'groupe'\n",
    "DATA_MODE = 'groupe_aligned'\n",
    "# DATA_MODE = 'groupe_multiclass'\n",
    "\n",
    "class MetricMonitor:\n",
    "    def __init__(self, name, float_precision=3):\n",
    "        self.name = name\n",
    "        self.float_precision = float_precision\n",
    "        self.metrics = defaultdict(lambda: {\"value\": 0, \"count\": 0, \"avg\": 0})\n",
    "\n",
    "    def update(self, metric_name, value, weight=1.):\n",
    "        metric = self.metrics[metric_name]\n",
    "        metric[\"value\"] += value * weight\n",
    "        metric[\"count\"] += 1 * weight\n",
    "        metric[\"avg\"] = metric[\"value\"] / (1e-7 + metric[\"count\"])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name,\n",
    "                    avg=metric[\"avg\"],\n",
    "                    float_precision=self.float_precision) for (\n",
    "                    metric_name,\n",
    "                    metric) in self.metrics.items()])\n",
    "\n",
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    frame_list = []\n",
    "    mask_list = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        frame_data, mask_data, target_data = sample\n",
    "        frame_list.append(frame_data)\n",
    "        targets.append(torch.from_numpy(target_data))\n",
    "        mask_list.append(torch.from_numpy(mask_data))\n",
    "\n",
    "        # import cv2\n",
    "        # a = np.clip(np.transpose(128 + frame_data.cpu().detach().numpy(), (1, 2, 0)), 0, 256).astype(np.uint8)\n",
    "        # print(target_data.shape)\n",
    "\n",
    "    return torch.stack(frame_list, 0), torch.stack(mask_list, 0), targets\n",
    "\n",
    "\n",
    "# Function to compute mAP\n",
    "def compute_map(detections, annotations, iou_threshold=0.5):\n",
    "    aps = []\n",
    "    for det, ann in zip(detections, annotations):\n",
    "        detected = [False] * len(det)\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        for a in ann:\n",
    "            matched = False\n",
    "            for d in det:\n",
    "                iou = compute_iou(d[:4], a)\n",
    "                if iou >= iou_threshold:\n",
    "                    true_positive += 1\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                false_positive += 1\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / len(ann) if len(ann) > 0 else 0\n",
    "        aps.append(precision * recall)\n",
    "\n",
    "    return np.mean(aps)\n",
    "\n",
    "\n",
    "# Function to compute IoU\n",
    "def compute_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "\n",
    "    xi1 = max(x1, x1g)\n",
    "    yi1 = max(y1, y1g)\n",
    "    xi2 = min(x2, x2g)\n",
    "    yi2 = min(y2, y2g)\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x2g - x1g) * (y2g - y1g)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area\n",
    "\n",
    "# Function to compute precision, recall, F1 score, and IoU\n",
    "def compute_metrics(detections, annotations, iou_threshold=0.5):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for det, ann in zip(detections, annotations):\n",
    "        detected = [False] * len(det)\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        false_negative = len(ann)\n",
    "\n",
    "        for a in ann:\n",
    "            matched = False\n",
    "            for d in det:\n",
    "                iou = compute_iou(d[:4], a)\n",
    "                if iou >= iou_threshold:\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                true_positive += 1\n",
    "                false_negative -= 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / len(ann) if len(ann) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        y_true.extend([1] * len(ann) + [0] * false_positive)\n",
    "        y_pred.extend([1] * true_positive + [0] * (false_positive + false_negative))\n",
    "\n",
    "    precision_avg, recall_avg, f1_score_avg, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "\n",
    "    return precision_avg, recall_avg, f1_score_avg\n",
    "\n",
    "\n",
    "def validation_epoch(model, dataloader, device, visualiation_count=4):\n",
    "    model.eval()\n",
    "    torch.enable_grad(False)\n",
    "\n",
    "    visualiation_frame_show_list = []\n",
    "\n",
    "    annotations = []\n",
    "    detections = []\n",
    "    for item_batch in dataloader:\n",
    "        for item in zip(*tuple(item_batch)):\n",
    "            frame_data, mask_ground_truth, ground_truth = item\n",
    "            ground_truth[:, :-1] = (ground_truth[:, :-1].reshape(ground_truth.shape[0], -1, 2) * \\\n",
    "                                    torch.tensor(frame_data.shape[1:][::-1])).reshape(ground_truth[:, :-1].shape)\n",
    "            output = model(torch.permute(frame_data[None, ...], (0, 2, 3, 1)).to(device))\n",
    "            confidence_list, landmark_list, bbox_lsit, mask_predicted = \\\n",
    "                [item.detach().cpu().numpy() for item in output]\n",
    "            annotations.append(ground_truth[:, :4].detach().cpu().numpy())\n",
    "            detections.append(bbox_lsit)\n",
    "\n",
    "            if len(visualiation_frame_show_list) < visualiation_count:\n",
    "                frame_data = torch.permute(frame_data, (1, 2, 0)).contiguous().detach().byte().cpu().numpy()\n",
    "                mask_ground_truth = (255 * torch.nn.functional.one_hot(mask_ground_truth.long()))[0].detach().cpu().numpy()\n",
    "                mask_predicted = (255 * np.transpose(mask_predicted, (0, 2, 3, 1))[0]).astype(np.uint8)\n",
    "\n",
    "                for confidence, box, landmark in zip(confidence_list, bbox_lsit, landmark_list):\n",
    "                    label_index = int(confidence.argmax().item())\n",
    "                    label_confidence = confidence[label_index].item()\n",
    "                    label_show = int(max(label_index - 1, 0))\n",
    "\n",
    "                    _box = list(map(int, box + 0.5))\n",
    "                    _landm = list(map(int, landmark + 0.5))\n",
    "                    cv2.rectangle(frame_data, (_box[0], _box[1]), (_box[2], _box[3]), (0, 0, 255), 2)\n",
    "\n",
    "                    cx = _box[0]\n",
    "                    cy = _box[1] + 12\n",
    "                    text = f\"{label_confidence:.4f}\"\n",
    "                    cv2.putText(frame_data, text, (cx, cy),\n",
    "                                cv2.FONT_HERSHEY_DUPLEX, 0.75, (255, 255, 0), thickness=2)\n",
    "\n",
    "                    cx = _box[0]\n",
    "                    cy = _box[1] + 48\n",
    "                    text = f\"[{label_show}]\"\n",
    "                    cv2.putText(frame_data, text, (cx, cy),\n",
    "                                cv2.FONT_HERSHEY_DUPLEX, 0.75, (196, 196, 0), thickness=2)\n",
    "\n",
    "                    # landms\n",
    "                    cv2.circle(frame_data, (_landm[0], _landm[1]), 1, (0, 0, 255), 4)\n",
    "                    cv2.circle(frame_data, (_landm[2], _landm[3]), 1, (0, 255, 255), 4)\n",
    "                    cv2.circle(frame_data, (_landm[4], _landm[5]), 1, (255, 0, 255), 4)\n",
    "                    cv2.circle(frame_data, (_landm[6], _landm[7]), 1, (0, 255, 0), 4)\n",
    "                    cv2.circle(frame_data, (_landm[8], _landm[9]), 1, (255, 0, 0), 4)\n",
    "\n",
    "                frame_show = np.vstack([frame_data, mask_ground_truth, mask_predicted])\n",
    "                visualiation_frame_show_list.append(frame_show)\n",
    "\n",
    "    frame_show = np.hstack(visualiation_frame_show_list)\n",
    "    cv2.imwrite(F'frame_show_validation_{DATA_MODE}.jpg', frame_show)\n",
    "\n",
    "    # Calculate mAP\n",
    "    mAP = compute_map(detections, annotations)\n",
    "    print(f\"Mean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision, recall, f1_score = compute_metrics(detections, annotations)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return mAP, precision, recall, f1_score\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = argparse.ArgumentParser(description='Retinaface Training')\n",
    "    parser.add_argument('--training_dataset', default='./data/widerface/train/label.txt', help='Training dataset directory')\n",
    "    parser.add_argument('--network', default='mobile0.25', help='Backbone network mobile0.25 or resnet50')\n",
    "    parser.add_argument('--num_workers', default=4, type=int, help='Number of workers used in dataloading')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, help='momentum')\n",
    "    parser.add_argument('--resume_net', default=None, help='resume net for retraining')\n",
    "    parser.add_argument('--resume_epoch', default=0, type=int, help='resume iter for retraining')\n",
    "    parser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay for SGD')\n",
    "    parser.add_argument('--gamma', default=0.1, type=float, help='Gamma update for SGD')\n",
    "    parser.add_argument('--save_folder', default='./weights/', help='Location to save checkpoint models')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.save_folder):\n",
    "        os.mkdir(args.save_folder)\n",
    "    cfg = None\n",
    "    if args.network == \"mobile0.25\":\n",
    "        cfg = cfg_mnet\n",
    "    elif args.network == \"resnet50\":\n",
    "        cfg = cfg_re50\n",
    "\n",
    "    # num_classes = 11\n",
    "    # use_batch_normalization = True\n",
    "    # multiclass = num_classes > 2\n",
    "\n",
    "    img_dim = cfg['image_size']\n",
    "    num_gpu = cfg['ngpu']\n",
    "    batch_size = cfg['batch_size']\n",
    "    max_epoch = cfg['epoch']\n",
    "    gpu_train = cfg['gpu_train']\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    num_workers = args.num_workers\n",
    "    momentum = args.momentum\n",
    "    weight_decay = args.weight_decay\n",
    "    initial_lr = args.lr\n",
    "    gamma = args.gamma\n",
    "    df_train = args.training_dataset\n",
    "    save_folder = args.save_folder\n",
    "\n",
    "    transform_train = A.Compose([\n",
    "        A.OneOf([\n",
    "            A.ImageCompression(compression_type='jpeg', quality_range=(50, 100), p=1.),\n",
    "            A.ImageCompression(compression_type='webp', quality_range=(50, 100), p=1.),\n",
    "        ], p=0.15),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3, p=1),\n",
    "            A.MedianBlur(p=1),\n",
    "            A.GridDistortion(p=1)\n",
    "            ], p=0.15),\n",
    "        A.OneOf([\n",
    "            A.HueSaturationValue(p=1),\n",
    "            A.RandomBrightnessContrast(p=1),\n",
    "            A.RandomGamma(gamma_limit=(20, 180), p=1),\n",
    "            A.ColorJitter(p=1)\n",
    "            ], p=1.),\n",
    "        A.OneOf([\n",
    "            A.ToGray(p=1),\n",
    "            A.ToSepia(p=1)\n",
    "            ], p=0.15),\n",
    "        A.OneOf([\n",
    "            A.Affine(scale=(0.9, 1.1), rotate=(-180, 180), translate_percent=(0.025, 0.025), border=cv2.BORDER_REPLICATE, p=1.),\n",
    "            A.Affine(scale=(0.9, 1.1), rotate=(-180, 180), translate_percent=(0.025, 0.025), border=cv2.BORDER_CONSTANT, cval=(0, 255), p=1.),\n",
    "            ], p=0.9),\n",
    "        A.Compose([\n",
    "            A.OneOf([\n",
    "                A.Resize(128, 128, p=1),\n",
    "                A.Resize(196, 196, p=1),\n",
    "                A.Resize(256, 256, p=1),\n",
    "                A.Resize(384, 384, p=1),\n",
    "                A.Resize(512, 512, p=1)], p=1),\n",
    "            A.Resize(640, 640, p=1),\n",
    "            ], p=0.25),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']),\n",
    "        keypoint_params=A.KeypointParams(format='xy', remove_invisible=False, angle_in_degrees=False,\n",
    "                                         check_each_transform=False))\n",
    "\n",
    "    transform_validation = None\n",
    "\n",
    "    if DATA_MODE == 'single':\n",
    "        df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat/dataframe_digit_2025.03.10.csv.zip\")\n",
    "        df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat_year/dataframe_digit_2025.03.10.csv.zip\")\n",
    "        _Dataset = CustomDetectionDataset\n",
    "    elif DATA_MODE == 'groupe':\n",
    "        df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat/dataframe_year_digit_2025.02.15_fix.csv.zip\")\n",
    "        df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat_year/dataframe_year_digit_2025_fix.03.10.csv.zip\")\n",
    "        _Dataset = GroupeDetectionDataset\n",
    "    elif DATA_MODE == 'groupe_multiclass':\n",
    "        num_classes = 1 + 3   # backgroud - 0, d4 - 1, d2 - 2, d1 - 3\n",
    "        df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/__ucoin_dataset_cache/df_year_region_detect_train_2025.04.12.csv.zip\")\n",
    "        df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/__ucoin_dataset_cache/df_year_region_detect_test_2025.04.12.csv.zip\")\n",
    "        _Dataset = GroupeAlignedMulticlassDetectionDataset\n",
    "\n",
    "        weight_j = 20\n",
    "        weight_b = 1\n",
    "\n",
    "    elif DATA_MODE == \"groupe_aligned\":\n",
    "\n",
    "        num_classes = 11\n",
    "        weight_j = 10\n",
    "        weight_b = 10\n",
    "\n",
    "        df_train = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat/dataframe_year_digit_2025.02.15_fix.csv.zip\")\n",
    "        df_validation = pd.read_csv(\"/home/ubuntu/projects/dataset/cvat_year/dataframe_year_digit_2025.03.10_fix.csv.zip\")\n",
    "        _Dataset = GroupeAlignedDetectionDataset\n",
    "\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    use_batch_normalization = True\n",
    "    multiclass = num_classes > 2\n",
    "\n",
    "    if use_batch_normalization:\n",
    "        rgb_mean = (0, 0, 0)\n",
    "    else:\n",
    "        rgb_mean = (104, 117, 123) # bgr order\n",
    "\n",
    "    save_folder = os.path.join(os.path.abspath(save_folder), '', DATA_MODE)\n",
    "\n",
    "    cfg['pretrain'] = False\n",
    "    max_epoch = 2048\n",
    "    device = 'cpu'\n",
    "    num_gpu = -1\n",
    "\n",
    "    model = UNetRetinaConcat(cfg=cfg, use_batch_normalization=use_batch_normalization, num_classes=num_classes)\n",
    "    print(\"Printing net...\")\n",
    "    print(model)\n",
    "\n",
    "    if args.resume_net is not None:\n",
    "        model.load_state_dict(torch.load(args.resume_net))\n",
    "\n",
    "    if num_gpu > 1 and gpu_train:\n",
    "        model = torch.nn.DataParallel(model).to(device)\n",
    "    else:\n",
    "        model = model.to(device)\n",
    "        \n",
    "    config_name = cfg['name']\n",
    "    export_config = {key: cfg[key] for key in ['variance']}\n",
    "    export_config['nms_threshold'] = 0.35\n",
    "    export_config['confidence_threshold'] = 0.02\n",
    "    export_config['top_k'] = 512\n",
    "    export_config['color_scheme'] = 'BGR'\n",
    "    export_config['mean'] = rgb_mean\n",
    "\n",
    "    bounding_box_from_points = False\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=32, threshold=1e-4,\n",
    "                                                           factor=0.5, min_lr=1e-6, verbose=1)\n",
    "\n",
    "    criterion_unet_j = JaccardLoss(mode='multiclass', from_logits=True, smooth=32)\n",
    "    criterion_unet_b = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    export_model = RetinaStaticExportWrapperV2(model, export_config, bounding_box_from_points, return_mask=True)\n",
    "    export_model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    epoch = 0 + args.resume_epoch\n",
    "    print('Loading Dataset...')\n",
    "\n",
    "    dataset_train = _Dataset(df_train, preproc(img_dim, rgb_mean, use_mirror=False, pre_scales=[1.0],\n",
    "                                               transform=transform_train), multiclass=multiclass)\n",
    "\n",
    "    dataset_validation = _Dataset(df_validation, preproc(img_dim, rgb_mean, use_mirror=False, pre_scales=[1.0],\n",
    "                                                         transform=transform_validation), multiclass=multiclass)\n",
    "\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=32,\n",
    "                                  num_workers=num_workers, collate_fn=detection_collate)\n",
    "    dataloader_validation = DataLoader(dataset_validation, batch_size=32, shuffle=True,\n",
    "                                       num_workers=1, collate_fn=detection_collate)\n",
    "\n",
    "    epoch_size = math.ceil(len(dataset_train) / batch_size)\n",
    "    max_iter = max_epoch * epoch_size\n",
    "\n",
    "    stepvalues = (cfg['decay1'] * epoch_size, cfg['decay2'] * epoch_size)\n",
    "    step_index = 0\n",
    "\n",
    "    if args.resume_epoch > 0:\n",
    "        start_iter = args.resume_epoch * epoch_size\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    iteration = 0\n",
    "\n",
    "    # for epoch in range(max_epoch):\n",
    "    #     for i in range(len()):\n",
    "\n",
    "    f1_score_max = -np.inf\n",
    "    mode = 'Train'\n",
    "    summary_writer_path = f'/home/ubuntu/projects/results/logs/detection-{DATA_MODE}-{config_name}/{datetime.now().isoformat()}'\n",
    "    summary_writer = SummaryWriter(summary_writer_path)\n",
    "\n",
    "    _datetime = datetime.now().isoformat()\n",
    "    train_monitor = MetricMonitor('Train')\n",
    "\n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        if iteration % epoch_size == 0:\n",
    "\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(data.DataLoader(dataset_train, batch_size, shuffle=True,\n",
    "                                                  num_workers=num_workers, collate_fn=detection_collate))\n",
    "            # if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > cfg['decay1']):\n",
    "            #     torch.save(model.state_dict(), save_folder + '_epoch_' + str(epoch) + '.pth')\n",
    "            epoch += 1\n",
    "\n",
    "            mode = 'Train'\n",
    "            for key, value in train_monitor.metrics.items():\n",
    "                summary_writer.add_scalar(f'{mode}-{key}', value['avg'], iteration)\n",
    "\n",
    "            train_monitor = MetricMonitor('Train')\n",
    "\n",
    "            export_model.eval()\n",
    "            model.eval()\n",
    "            export_model.model.phase = 'test'\n",
    "            mAP, precision, recall, f1_score = validation_epoch(export_model, dataloader_validation, device)\n",
    "            export_model.model.phase = 'train'\n",
    "            export_model.train()\n",
    "            model.train()\n",
    "            torch.enable_grad(False)\n",
    "\n",
    "            if f1_score > f1_score_max:\n",
    "                _best_model_path = os.path.join(save_folder, '', f'{config_name}_epoch_{str(epoch).zfill(4)}_mAP{mAP:.5f}_F1_{f1_score:.5f}.pth')\n",
    "                os.makedirs(os.path.dirname(_best_model_path), exist_ok=True)\n",
    "                print(f'Best Model :: {_best_model_path}')\n",
    "                torch.save(model.state_dict(), _best_model_path)\n",
    "                f1_score_max = f1_score\n",
    "\n",
    "            mode = 'Validation'\n",
    "            for key, value in zip(['mAP', 'precision', 'recall', 'f1_score'],\n",
    "                                  [mAP, precision, recall, f1_score]):\n",
    "                summary_writer.add_scalar(f'{mode}-{key}', value, iteration)\n",
    "\n",
    "            lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            summary_writer.add_scalar('LR', lr, iteration)\n",
    "\n",
    "            scheduler.step(f1_score)\n",
    "\n",
    "            _last_model_path = os.path.join(save_folder, '', f'{config_name}_{_datetime}_Final.pth')\n",
    "            os.makedirs(os.path.dirname(_last_model_path), exist_ok=True)\n",
    "            torch.save(model.state_dict(), _last_model_path)\n",
    "\n",
    "        load_t0 = time.time()\n",
    "        # if iteration in stepvalues:\n",
    "        #     step_index += 1\n",
    "        # lr = adjust_learning_rate(initial_lr, optimizer, gamma, epoch, step_index, iteration, epoch_size)\n",
    "\n",
    "        # load train data\n",
    "        images, mask_true, targets = next(batch_iterator)\n",
    "        images = images.to(device)\n",
    "        mask_true = mask_true.to(device)\n",
    "        targets = [anno.to(device) for anno in targets]\n",
    "\n",
    "        # forward\n",
    "        out, mask_predicted = model(images)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
    "        # loss_j = criterion_unet_b(mask_predicted, mask_true[:, 0, ...].long()) + criterion_unet_j(mask_predicted[:, 1:, ...], torch.clamp(mask_true.long() - 1, 0, 1))\n",
    "        loss_j = criterion_unet_j(mask_predicted, mask_true.long())\n",
    "        loss_b = criterion_unet_b(mask_predicted, mask_true[:, 0, ...].long())\n",
    "\n",
    "        loss = cfg['loc_weight'] * loss_l + 10 * loss_c + loss_landm + weight_j * loss_j + weight_b * loss_b\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_monitor.update(\"Landmarks\", loss_landm.item())\n",
    "        train_monitor.update(\"Confidence\", loss_c.item())\n",
    "        train_monitor.update(\"Location\", loss_l.item())\n",
    "        train_monitor.update(\"Jacard\", loss_j.item())\n",
    "        train_monitor.update(\"CCE\", loss_b.item())\n",
    "\n",
    "        if iteration % epoch_size == 0:\n",
    "            item_count = 4\n",
    "            # frame_data = np.transpose(images[0:item_count].detach().cpu().numpy(), (0, 2, 3, 1)) * 255 / (images.max() - images.min()).item()\n",
    "            frame_data = (np.transpose(images[0:item_count].detach().cpu().numpy(), (0, 2, 3, 1)))\n",
    "            mask_true_data = 255 * torch.nn.functional.one_hot(mask_true[0:item_count, 0].long().detach()).cpu().numpy()\n",
    "            mask_predicted_data = 255 * np.transpose(torch.softmax(mask_predicted[0:item_count], dim=1).detach().cpu().numpy(), (0, 2, 3, 1))\n",
    "\n",
    "            frame_show = np.vstack([np.hstack(frame_data), np.hstack(mask_true_data), np.hstack(mask_predicted_data)])\n",
    "            cv2.imwrite(f'frame_show_{DATA_MODE}.jpg', frame_show)\n",
    "\n",
    "        load_t1 = time.time()\n",
    "        batch_time = load_t1 - load_t0\n",
    "        eta = int(batch_time * (max_iter - iteration))\n",
    "        print('Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || Loc: {:.4f} Cla: {:.4f} Landm: {:.4f} Mask: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}'\n",
    "              .format(epoch, max_epoch, (iteration % epoch_size) + 1,\n",
    "              epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), loss_landm.item(), loss_j.item(), lr, batch_time, str(timedelta(seconds=eta))))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(initial_lr, optimizer, gamma, epoch, step_index, iteration, epoch_size):\n",
    "    \"\"\"Sets the learning rate\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    warmup_epoch = -1\n",
    "    if epoch <= warmup_epoch:\n",
    "        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)\n",
    "    else:\n",
    "        lr = initial_lr * (gamma ** (step_index))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
